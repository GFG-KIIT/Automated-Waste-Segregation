{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "271f029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import imageio\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import tensorflow.keras as K\n",
    "import tensorflow.keras.backend as Kback\n",
    "\n",
    "from keras import layers\n",
    "from keras.layers import Input\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.applications.resnet import ResNet50\n",
    "from keras.applications import vgg16\n",
    "from keras.applications import inception_v3\n",
    "from keras.src.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from keras.applications import DenseNet121, NASNetMobile, EfficientNetB0, Xception\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.applications.densenet import DenseNet121\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c01fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for importing the zip file\n",
    "import zipfile\n",
    "\n",
    "# Specifying the path to the zip file\n",
    "zip_file_path = 'waste_classfication_data.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59b6b3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile \n",
    "\n",
    "# Replace the path to your zip file and the directory where you want to extract the contents\n",
    "zip_file_path = r\"C:\\Users\\KIIT\\Downloads\\waste segregation\\garbage_classification.zip\"\n",
    "extracted_dir = r\"C:\\Users\\KIIT\\Downloads\\waste segregation\\extracted_data\" # Specify the directory where you want to extract\n",
    "\n",
    "# Create a zip object and extract the contents to the specified directory\n",
    "with ZipFile(zip_file_path, 'r') as zObject:\n",
    "    zObject.extractall(path=extracted_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a96acef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class battery: 945 samples\n",
      "Class biological: 985 samples\n",
      "Class brown-glass: 607 samples\n",
      "Class cardboard: 891 samples\n",
      "Class clothes: 5325 samples\n",
      "Class green-glass: 629 samples\n",
      "Class metal: 769 samples\n",
      "Class paper: 1050 samples\n",
      "Class plastic: 865 samples\n",
      "Class shoes: 1977 samples\n",
      "Class trash: 697 samples\n",
      "Class white-glass: 775 samples\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the extracted directory (update this path as needed)\n",
    "extracted_dir = \"C:/Users/KIIT/Downloads/waste segregation/extracted_data/garbage_classification\"  # Update this to your actual path\n",
    "\n",
    "# Define class names (folder names)\n",
    "class_names = ['battery', 'biological', 'brown-glass', 'cardboard', 'clothes', 'green-glass', 'metal', 'paper', 'plastic', 'shoes', 'trash', 'white-glass']\n",
    "\n",
    "# Initialize dictionary to store class sizes\n",
    "class_sizes = {}\n",
    "\n",
    "# Iterate over each class and count the number of files\n",
    "for class_name in class_names:\n",
    "    class_path = os.path.join(extracted_dir, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "        num_files = len(os.listdir(class_path))\n",
    "        class_sizes[class_name] = num_files\n",
    "    else:\n",
    "        class_sizes[class_name] = 0\n",
    "        print(f\"Directory not found: {class_path}\")  # Debug print\n",
    "\n",
    "# Print the size of each class\n",
    "for class_name, size in class_sizes.items():\n",
    "    print(f\"Class {class_name}: {size} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc8273dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class battery: 945 images with shapes [(280, 180), (220, 165), (275, 183), (275, 183), (224, 224)] ...\n",
      "Class biological: 985 images with shapes [(236, 214), (337, 150), (263, 192), (275, 183), (277, 182)] ...\n",
      "Class brown-glass: 607 images with shapes [(266, 189), (230, 218), (224, 224), (224, 224), (225, 225)] ...\n",
      "Class cardboard: 891 images with shapes [(512, 384), (512, 384), (512, 384), (512, 384), (512, 384)] ...\n",
      "Class clothes: 5325 images with shapes [(400, 534), (400, 533), (400, 534), (400, 533), (400, 533)] ...\n",
      "Class green-glass: 629 images with shapes [(200, 200), (275, 183), (224, 224), (179, 282), (204, 247)] ...\n",
      "Class metal: 769 images with shapes [(259, 194), (225, 225), (512, 384), (266, 190), (512, 384)] ...\n",
      "Class paper: 1050 images with shapes [(275, 183), (512, 384), (205, 246), (512, 384), (512, 384)] ...\n",
      "Class plastic: 865 images with shapes [(194, 259), (512, 384), (512, 384), (512, 384), (259, 194)] ...\n",
      "Class shoes: 1977 images with shapes [(400, 533), (400, 534), (220, 147), (228, 221), (225, 225)] ...\n",
      "Class trash: 697 images with shapes [(276, 183), (259, 194), (225, 225), (225, 225), (259, 194)] ...\n",
      "Class white-glass: 775 images with shapes [(285, 177), (512, 384), (512, 384), (322, 156), (512, 384)] ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Define the extracted directory (update this path as needed)\n",
    "extracted_dir = \"C:/Users/KIIT/Downloads/waste segregation/extracted_data/garbage_classification\"  # Update this to your actual path\n",
    "\n",
    "# Define class names (folder names)\n",
    "class_names = ['battery', 'biological', 'brown-glass', 'cardboard', 'clothes', 'green-glass', 'metal', 'paper', 'plastic', 'shoes', 'trash', 'white-glass']\n",
    "\n",
    "# Initialize dictionary to store class shapes\n",
    "class_shapes = {}\n",
    "\n",
    "# Iterate over each class and calculate the shape of images\n",
    "for class_name in class_names:\n",
    "    class_path = os.path.join(extracted_dir, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "        shapes = []\n",
    "        for file_name in os.listdir(class_path):\n",
    "            file_path = os.path.join(class_path, file_name)\n",
    "            if os.path.isfile(file_path):\n",
    "                try:\n",
    "                    with Image.open(file_path) as img:\n",
    "                        shapes.append(img.size)  # img.size returns (width, height)\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not open {file_path}: {e}\")\n",
    "        class_shapes[class_name] = shapes\n",
    "    else:\n",
    "        class_shapes[class_name] = []\n",
    "        print(f\"Directory not found: {class_path}\")  # Debug print\n",
    "\n",
    "# Print the shape of images in each class\n",
    "for class_name, shapes in class_shapes.items():\n",
    "    print(f\"Class {class_name}: {len(shapes)} images with shapes {shapes[:5]} ...\")  # Print first 5 shapes for brevity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ea0c561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resizing completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "# Define the extracted directory (update this path as needed)\n",
    "extracted_dir = \"C:/Users/KIIT/Downloads/waste segregation/extracted_data/garbage_classification\"\n",
    "resized_dir = \"C:/Users/KIIT/Downloads/waste segregation/resized_dir\"  # New directory for resized images\n",
    "\n",
    "# Create the new directory structure\n",
    "if not os.path.exists(resized_dir):\n",
    "    os.makedirs(resized_dir)\n",
    "\n",
    "# Define class names (folder names)\n",
    "class_names = ['battery', 'biological', 'brown-glass', 'cardboard', 'clothes', 'green-glass', 'metal', 'paper', 'plastic', 'shoes', 'trash', 'white-glass']\n",
    "\n",
    "# Define the target size\n",
    "target_size = (128, 128)\n",
    "\n",
    "# Resize images and save them to the new directory\n",
    "for class_name in class_names:\n",
    "    \n",
    "    class_path = os.path.join(extracted_dir, class_name)\n",
    "    resized_class_path = os.path.join(resized_dir, class_name)\n",
    "    \n",
    "    if not os.path.exists(resized_class_path):\n",
    "        os.makedirs(resized_class_path)\n",
    "\n",
    "    if os.path.isdir(class_path):\n",
    "        for filename in os.listdir(class_path):\n",
    "            if filename.endswith('.jpeg') or filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "                file_path = os.path.join(class_path, filename)\n",
    "                img = cv2.imread(file_path)\n",
    "                if img is not None:\n",
    "                    resized_img = cv2.resize(img, target_size)\n",
    "                    output_path = os.path.join(resized_class_path, filename)\n",
    "                    cv2.imwrite(output_path, resized_img)\n",
    "                else:\n",
    "                    print(f\"Failed to load image: {file_path}\")\n",
    "    else:\n",
    "        print(f\"Directory not found: {class_path}\")\n",
    "\n",
    "print(\"Image resizing completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d41f7625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class battery: 945 images with shapes [(128, 128), (128, 128), (128, 128), (128, 128), (128, 128)] ...\n",
      "Class biological: 985 images with shapes [(128, 128), (128, 128), (128, 128), (128, 128), (128, 128)] ...\n",
      "Class brown-glass: 607 images with shapes [(128, 128), (128, 128), (128, 128), (128, 128), (128, 128)] ...\n",
      "Class cardboard: 891 images with shapes [(128, 128), (128, 128), (128, 128), (128, 128), (128, 128)] ...\n",
      "Class clothes: 5325 images with shapes [(128, 128), (128, 128), (128, 128), (128, 128), (128, 128)] ...\n",
      "Class green-glass: 629 images with shapes [(128, 128), (128, 128), (128, 128), (128, 128), (128, 128)] ...\n",
      "Class metal: 769 images with shapes [(128, 128), (128, 128), (128, 128), (128, 128), (128, 128)] ...\n",
      "Class paper: 1050 images with shapes [(128, 128), (128, 128), (128, 128), (128, 128), (128, 128)] ...\n",
      "Class plastic: 865 images with shapes [(128, 128), (128, 128), (128, 128), (128, 128), (128, 128)] ...\n",
      "Class shoes: 1977 images with shapes [(128, 128), (128, 128), (128, 128), (128, 128), (128, 128)] ...\n",
      "Class trash: 697 images with shapes [(128, 128), (128, 128), (128, 128), (128, 128), (128, 128)] ...\n",
      "Class white-glass: 775 images with shapes [(128, 128), (128, 128), (128, 128), (128, 128), (128, 128)] ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "resized_dir = \"C:/Users/KIIT/Downloads/waste segregation/resized_dir\"\n",
    "\n",
    "# Define class names (folder names)\n",
    "class_names = ['battery', 'biological', 'brown-glass', 'cardboard', 'clothes', 'green-glass', 'metal', 'paper', 'plastic', 'shoes', 'trash', 'white-glass']\n",
    "\n",
    "# Initialize dictionary to store class shapes\n",
    "class_shapes = {}\n",
    "\n",
    "# Iterate over each class and calculate the shape of images\n",
    "for class_name in class_names:\n",
    "    class_path = os.path.join(resized_dir, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "        shapes = []\n",
    "        for file_name in os.listdir(class_path):\n",
    "            file_path = os.path.join(class_path, file_name)\n",
    "            if os.path.isfile(file_path):\n",
    "                try:\n",
    "                    with Image.open(file_path) as img:\n",
    "                        shapes.append(img.size)  # img.size returns (width, height)\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not open {file_path}: {e}\")\n",
    "        class_shapes[class_name] = shapes\n",
    "    else:\n",
    "        class_shapes[class_name] = []\n",
    "        print(f\"Directory not found: {class_path}\")  # Debug print\n",
    "\n",
    "# Print the shape of images in each class\n",
    "for class_name, shapes in class_shapes.items():\n",
    "    print(f\"Class {class_name}: {len(shapes)} images with shapes {shapes[:5]} ...\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1369f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists for images and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Load images and assign labels\n",
    "for label, class_name in enumerate(class_names):\n",
    "    class_path = os.path.join(resized_dir, class_name)\n",
    "    image_files = os.listdir(class_path)\n",
    "\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(class_path, image_file)\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_COLOR)  # Read as GBR\n",
    "        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "        img = img / 255.0  # Normalize pixel values (assuming 8-bit images)\n",
    "        images.append(img)\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51e84b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 15515\n",
      "labels counts: Counter({4: 5325, 9: 1977, 7: 1050, 1: 985, 0: 945, 3: 891, 8: 865, 11: 775, 6: 769, 10: 697, 5: 629, 2: 607})\n"
     ]
    }
   ],
   "source": [
    "images, labels = shuffle(images, labels, random_state=42)\n",
    "\n",
    "print('Data length:', len(images))\n",
    "print('labels counts:', Counter(labels))\n",
    "\n",
    "images = np.array(images).reshape(-1, 128, 128, 3)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb9232c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length: 10860 (10860, 128, 128, 3)\n",
      "Valid length: 2327 (2328, 128, 128, 3)\n",
      "Test length: 2328 (2327, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(images, labels, test_size=0.3)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5)\n",
    "print('Train length:', len(X_train), X_train.shape)\n",
    "print('Valid length:', len(X_test), X_val.shape)\n",
    "print('Test length:', len(X_val), X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b4abea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNetV2(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(128,128,3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02cac0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84645d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(labels)\n",
    "Model_1 = Sequential()\n",
    "Model_1.add(base_model)\n",
    "Model_1.add(MaxPooling2D())\n",
    "Model_1.add(layers.Flatten())\n",
    "Model_1.add(layers.Dense(256, activation='relu'))\n",
    "Model_1.add(layers.Dense(128, activation='relu'))\n",
    "Model_1.add(layers.Dense(64, activation='relu'))\n",
    "Model_1.add(layers.Dense(NUM_CLASSES, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af0d7057",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=keras.optimizers.Adam(learning_rate=0.001)\n",
    "losses=keras.losses.SparseCategoricalCrossentropy()\n",
    "metrics=['accuracy']\n",
    "Model_1.compile(optimizer=optimizer,loss=losses,metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54b4c260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "340/340 [==============================] - 241s 659ms/step - loss: 0.6453 - accuracy: 0.8333 - val_loss: 0.3360 - val_accuracy: 0.8935\n",
      "Epoch 2/20\n",
      "340/340 [==============================] - 185s 544ms/step - loss: 0.2175 - accuracy: 0.9265 - val_loss: 0.3681 - val_accuracy: 0.8991\n",
      "Epoch 3/20\n",
      "340/340 [==============================] - 210s 618ms/step - loss: 0.1151 - accuracy: 0.9621 - val_loss: 0.3677 - val_accuracy: 0.9042\n",
      "Epoch 4/20\n",
      "340/340 [==============================] - 186s 548ms/step - loss: 0.0763 - accuracy: 0.9750 - val_loss: 0.3514 - val_accuracy: 0.9158\n",
      "Epoch 5/20\n",
      "340/340 [==============================] - 168s 494ms/step - loss: 0.0504 - accuracy: 0.9842 - val_loss: 0.4830 - val_accuracy: 0.8965\n",
      "Epoch 6/20\n",
      "340/340 [==============================] - 156s 459ms/step - loss: 0.0535 - accuracy: 0.9824 - val_loss: 0.4471 - val_accuracy: 0.9098\n",
      "Epoch 7/20\n",
      "340/340 [==============================] - 161s 474ms/step - loss: 0.0428 - accuracy: 0.9857 - val_loss: 0.4146 - val_accuracy: 0.9085\n",
      "Epoch 8/20\n",
      "340/340 [==============================] - 158s 465ms/step - loss: 0.0506 - accuracy: 0.9839 - val_loss: 0.4233 - val_accuracy: 0.9167\n",
      "Epoch 9/20\n",
      "340/340 [==============================] - 156s 458ms/step - loss: 0.0345 - accuracy: 0.9899 - val_loss: 0.4752 - val_accuracy: 0.9128\n",
      "Epoch 10/20\n",
      "340/340 [==============================] - 156s 458ms/step - loss: 0.0199 - accuracy: 0.9943 - val_loss: 0.4912 - val_accuracy: 0.9154\n",
      "Epoch 11/20\n",
      "340/340 [==============================] - 157s 461ms/step - loss: 0.0373 - accuracy: 0.9896 - val_loss: 0.5249 - val_accuracy: 0.9055\n",
      "Epoch 12/20\n",
      "340/340 [==============================] - 161s 475ms/step - loss: 0.0379 - accuracy: 0.9883 - val_loss: 0.4304 - val_accuracy: 0.9119\n",
      "Epoch 13/20\n",
      "340/340 [==============================] - 160s 472ms/step - loss: 0.0160 - accuracy: 0.9953 - val_loss: 0.5184 - val_accuracy: 0.9154\n",
      "Epoch 14/20\n",
      "340/340 [==============================] - 160s 470ms/step - loss: 0.0326 - accuracy: 0.9916 - val_loss: 0.5758 - val_accuracy: 0.9016\n",
      "Epoch 15/20\n",
      "340/340 [==============================] - 162s 476ms/step - loss: 0.0307 - accuracy: 0.9906 - val_loss: 0.4841 - val_accuracy: 0.9210\n",
      "Epoch 16/20\n",
      "340/340 [==============================] - 162s 477ms/step - loss: 0.0213 - accuracy: 0.9935 - val_loss: 0.6120 - val_accuracy: 0.9124\n",
      "Epoch 17/20\n",
      "340/340 [==============================] - 159s 469ms/step - loss: 0.0332 - accuracy: 0.9913 - val_loss: 0.5007 - val_accuracy: 0.9145\n",
      "Epoch 18/20\n",
      "340/340 [==============================] - 163s 480ms/step - loss: 0.0094 - accuracy: 0.9971 - val_loss: 0.6127 - val_accuracy: 0.9102\n",
      "Epoch 19/20\n",
      "340/340 [==============================] - 159s 467ms/step - loss: 0.0217 - accuracy: 0.9946 - val_loss: 0.5180 - val_accuracy: 0.9244\n",
      "Epoch 20/20\n",
      "340/340 [==============================] - 157s 461ms/step - loss: 0.0366 - accuracy: 0.9901 - val_loss: 0.5534 - val_accuracy: 0.9107\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "history_Model_1 = Model_1.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe8032b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfc2621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
